import pymysql
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.cloud import language_v1
from google.cloud.language_v1 import Document
import os
import time
import random
from dotenv import load_dotenv
from datetime import datetime, date

# è®€å– .env è¨­å®šæª”
load_dotenv()

# å¾ç’°å¢ƒè®Šæ•¸ä¸­ç²å– API å¯†é‘°èˆ‡è³‡æ–™åº«è¨­å®š
API_KEY = os.getenv('YOUTUBE_API_KEY')
MYSQL_HOST = os.getenv('MARIADB_HOST')
MYSQL_USER = os.getenv('MARIADB_USER')
MYSQL_PASSWORD = os.getenv('MARIADB_PASSWORD')
MYSQL_DB = os.getenv('MARIADB_DB')

# è½‰æ› ISO 8601 æ ¼å¼ç‚º MySQL å¯ç”¨çš„ DATETIME æ ¼å¼
def convert_to_mysql_datetime(iso_datetime):
    dt = datetime.strptime(iso_datetime.replace('Z', ''), '%Y-%m-%dT%H:%M:%S')
    return dt.strftime('%Y-%m-%d %H:%M:%S')

# ä½¿ç”¨ Google Cloud Natural Language API é€²è¡Œæƒ…æ„Ÿåˆ†æ
def analyze_sentiment(text):
    if not text.strip():
        return 0.0
    client = language_v1.LanguageServiceClient()
    document = Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)
    try:
        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment
        return round(sentiment.score, 6)
    except Exception as e:
        print(f"âš ï¸ Google NLP API éŒ¯èª¤: {e}")
        return 0.0

# å»ºç«‹ MySQL é€£æ¥
def connect_to_db():
    try:
        conn = pymysql.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB,
            charset="utf8mb4"
        )
        return conn
    except pymysql.MySQLError as e:
        print(f"MySQL é€£ç·šéŒ¯èª¤: {e}")
        return None

# æª¢æŸ¥è³‡æ–™è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å‰µå»º
def create_tables_if_not_exist():
    conn = connect_to_db()
    if conn:
        try:
            cur = conn.cursor()
            cur.execute("SELECT 1 FROM information_schema.tables WHERE table_name = 'yt'")
            if not cur.fetchone():
                cur.execute("""
                    CREATE TABLE yt (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        video_id VARCHAR(255) NOT NULL,
                        title TEXT,
                        sentiment_score FLOAT(10, 6),
                        comment_content TEXT,
                        comment_sentiment_score FLOAT(10, 6),
                        site VARCHAR(50) NOT NULL,
                        search_keyword VARCHAR(100) NOT NULL,
                        capture_date DATE NOT NULL
                    )
                """)
            conn.commit()
        except pymysql.MySQLError as e:
            print(f"âŒ å»ºç«‹è³‡æ–™è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            conn.close()

# **å–®æ¢ç•™è¨€å³æ™‚å­˜å…¥è³‡æ–™åº«**
def save_to_db(video_id, title, sentiment_score, comment, site, search_keyword, capture_date):
    conn = connect_to_db()
    if conn:
        try:
            cur = conn.cursor()

            # æ’å…¥å–®æ¢ç•™è¨€
            video_query = """
                INSERT INTO yt (video_id, title, sentiment_score, comment_content, comment_sentiment_score, site, search_keyword, capture_date) 
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """
            video_params = (
                video_id,
                title,
                sentiment_score,
                comment['content'],  # åªå­˜å…¥å–®æ¢ç•™è¨€
                comment['sentiment_score'],
                site,
                search_keyword,
                capture_date
            )
            cur.execute(video_query, video_params)

            # ç«‹å³æäº¤åˆ°è³‡æ–™åº«
            conn.commit()
            print(f"âœ… æˆåŠŸå­˜å…¥ç•™è¨€ï¼š{comment['content'][:30]}... (å½±ç‰‡: {title[:30]})")

            # éš¨æ©Ÿå»¶é²é¿å… API éè¼‰
            time.sleep(random.uniform(1, 2))

        except pymysql.MySQLError as e:
            print(f"âŒ è³‡æ–™å„²å­˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            conn.close()

def youtube_scraper():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    create_tables_if_not_exist()

    # å–å¾—ä»Šå¤©çš„æ—¥æœŸ
    today = date.today().isoformat()

    # è®€å–é—œéµå­—
    with open('keywords_yt.txt', 'r') as file:
        keywords = file.readlines()

    for query in keywords:
        query = query.strip()
        print(f"ğŸ” æ­£åœ¨æœå°‹é—œéµå­—: {query}")

        # å–å¾—å½±ç‰‡
        videos = search_videos(query)
        time.sleep(random.uniform(3, 6))

        for video in videos:
            print(f"ğŸ“„ æ­£åœ¨çˆ¬å–å½±ç‰‡: {video['title']} ({video['video_id']})")

            # å–å¾—ç•™è¨€
            comments = get_all_comments(video['video_id'])
            time.sleep(random.uniform(2, 5))

            if not comments:
                print(f"âš ï¸ ç„¡æ³•ç²å–è©•è«–ï¼Œå½±ç‰‡ IDï¼š{video['video_id']}ï¼Œæ¨™é¡Œï¼š{video['title']}")
                continue

            # è¨ˆç®—å½±ç‰‡çš„å¹³å‡æƒ…æ„Ÿåˆ†æ•¸
            video_sentiment_score = sum([c['sentiment_score'] for c in comments]) / len(comments)

            # **å³æ™‚å­˜å…¥æ¯æ¢ç•™è¨€**
            for comment in comments:
                save_to_db(
                    video_id=video['video_id'],
                    title=video['title'],
                    sentiment_score=video_sentiment_score,  # å½±ç‰‡çš„ç¸½é«”æƒ…æ„Ÿåˆ†æ•¸
                    comment=comment,
                    site="youtube",
                    search_keyword=query,
                    capture_date=today
                )

            time.sleep(random.uniform(1, 3))

    print("âœ… æ‰€æœ‰è³‡æ–™å·²æˆåŠŸä¿å­˜è‡³è³‡æ–™åº«")

def search_videos(keyword, max_results=3):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    try:
        search_response = youtube.search().list(
            q=keyword,
            part='snippet',
            type='video',
            maxResults=max_results,
            order='date',
            videoDuration='medium',
            regionCode='TW'
        ).execute()

        return [{
            'video_id': item['id']['videoId'],
            'title': item['snippet'].get('title', 'No Title')
        } for item in search_response['items']]

    except HttpError as e:
        print(f"æœå°‹å¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯ï¼š{e}")
        return []

def get_all_comments(video_id, max_comments=50):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    comments = []
    try:
        request = youtube.commentThreads().list(part="snippet", videoId=video_id, maxResults=max_comments)
        response = request.execute()
        for item in response.get('items', []):
            text = item['snippet']['topLevelComment']['snippet'].get('textOriginal', '')
            comments.append({'content': text, 'sentiment_score': analyze_sentiment(text)})
    except HttpError:
        pass
    return comments

def main():
    youtube_scraper()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç¨‹å¼è¢«ä¸­æ–·ï¼ŒçµæŸåŸ·è¡Œã€‚")
