import requests
from bs4 import BeautifulSoup
import pymysql
import os
from dotenv import load_dotenv
from google.cloud import language_v1
from google.cloud.language_v1 import Document
from datetime import date

# è®€å–ç’°å¢ƒè®Šæ•¸
load_dotenv()
MYSQL_HOST = os.getenv('MARIADB_HOST')
MYSQL_USER = os.getenv('MARIADB_USER')
MYSQL_PASSWORD = os.getenv('MARIADB_PASSWORD')
MYSQL_DB = os.getenv('MARIADB_DB')

BASE_URL = "https://pttweb.tw/ALLPOST/*"  # åŸºæœ¬ URL
MAX_ARTICLES = 10  # æ¯å€‹é—œéµå­—æœ€å¤šæŠ“å– 10 ç¯‡æ–‡ç« 

# é€£æ¥ MariaDB
def connect_to_db():
    try:
        conn = pymysql.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB,
            charset="utf8mb4",
            autocommit=True
        )
        return conn
    except pymysql.MySQLError as e:
        print(f"âŒ MySQL é€£ç·šéŒ¯èª¤: {e}")
        return None

# ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨ï¼Œæ–°å¢ siteã€search_keyword èˆ‡ capture_date æ¬„ä½
def create_table():
    conn = connect_to_db()
    if conn:
        try:
            cur = conn.cursor()
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ptt (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    title TEXT NOT NULL,
                    content TEXT NOT NULL,
                    comment TEXT NOT NULL,
                    sentiment_score FLOAT(10, 6),
                    site VARCHAR(50) NOT NULL,
                    search_keyword VARCHAR(100) NOT NULL,
                    capture_date DATE NOT NULL
                )
            """)
            conn.commit()
            print("âœ… è³‡æ–™è¡¨æª¢æŸ¥å®Œæˆ")
        except pymysql.MySQLError as e:
            print(f"âŒ å»ºç«‹è³‡æ–™è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            conn.close()

# ä½¿ç”¨ Google Cloud Natural Language API é€²è¡Œæƒ…æ„Ÿåˆ†æ
def analyze_sentiment(text):
    if not text.strip():
        return 0.0
    
    client = language_v1.LanguageServiceClient()
    document = Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)
    
    try:
        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment
        return round(sentiment.score, 6)
    except Exception as e:
        print(f"âš ï¸ Google NLP API éŒ¯èª¤: {e}")
        return 0.0

# è®€å–é—œéµå­—
def load_keywords(filename="keywords.txt"):
    try:
        with open(filename, "r", encoding="utf-8") as file:
            return [line.strip() for line in file.readlines() if line.strip()]
    except FileNotFoundError:
        print(f"âŒ é—œéµå­—æª”æ¡ˆ {filename} ä¸å­˜åœ¨")
        return []

# æŠ“å–æ–‡ç« åˆ—è¡¨ï¼ˆåŠ å…¥ timeout èˆ‡ä¾‹å¤–è™•ç†ï¼‰
def fetch_article_links(keyword):
    search_url = f"{BASE_URL}{keyword}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(search_url, headers=headers, timeout=10)
    except requests.RequestException as e:
        print(f"âŒ é€£ç·šæœå°‹çµæœå¤±æ•—: {e}")
        return []

    if response.status_code != 200:
        print(f"âŒ ç„¡æ³•å–å¾—æœå°‹çµæœï¼ŒéŒ¯èª¤ç¢¼: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    articles = []
    for link in soup.select("div.articles a")[:MAX_ARTICLES]:
        title = link.select_one(".name").text.strip()
        url = f"https://pttweb.tw{link['href']}"
        articles.append({"title": title, "url": url})
    return articles

# è§£ææ–‡ç« å…§å®¹èˆ‡ç•™è¨€ï¼ˆåŠ å…¥ timeout èˆ‡ä¾‹å¤–è™•ç†ï¼‰
def parse_article(article_url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(article_url, headers=headers, timeout=10)
    except requests.RequestException as e:
        print(f"âŒ é€£ç·šæ–‡ç« å¤±æ•—: {e}")
        return None

    if response.status_code != 200:
        print(f"âŒ ç„¡æ³•ç²å–æ–‡ç« ï¼ŒéŒ¯èª¤ç¢¼: {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # æ“·å–æ¨™é¡Œ
    title_element = soup.select_one("div.article span.value h1")
    title = title_element.text.strip() if title_element else "No Title"

    # æ“·å–å…§æ–‡
    content_element = soup.select_one("div.article")
    content = content_element.text.strip() if content_element else "No Content"

    # æ“·å–ç•™è¨€
    comments_data = []
    for push in soup.select("div.push span.f3.push-content"):
        comment_text = push.text.strip()
        if comment_text:
            sentiment_score = analyze_sentiment(comment_text)
            comments_data.append({"comment": comment_text, "sentiment_score": sentiment_score})

    print(f"ğŸ“„ è§£ææ–‡ç« : {title[:30]} | å…§æ–‡é•·åº¦: {len(content)} | ç•™è¨€æ•¸é‡: {len(comments_data)}")
    return {"title": title, "content": content, "comments": comments_data}

# å„²å­˜è‡³ MariaDBï¼Œå…è¨±é‡è¤‡æ–‡ç« ï¼Œä¸¦æ–°å¢ capture_date æ¬„ä½
def save_to_db(title, content, comment, sentiment_score, site, search_keyword, capture_date):
    conn = connect_to_db()
    if conn:
        try:
            cur = conn.cursor()
            print(f"ğŸ”„ å˜—è©¦æ’å…¥è³‡æ–™:\næ¨™é¡Œ: {title[:30]}\nå…§æ–‡: {content[:50]}\nç•™è¨€: {comment[:50]}\næƒ…æ„Ÿåˆ†æ•¸: {sentiment_score}\nä¾†æº: {site}\né—œéµå­—: {search_keyword}\næŠ“å–æ—¥æœŸ: {capture_date}")
            sql_query = """
                INSERT INTO ptt (title, content, comment, sentiment_score, site, search_keyword, capture_date) 
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """
            cur.execute(sql_query, (title, content, comment, sentiment_score, site, search_keyword, capture_date))
            conn.commit()
            print(f"âœ… æˆåŠŸå„²å­˜: {title[:30]}...")
        except pymysql.MySQLError as e:
            print(f"âŒ MySQL éŒ¯èª¤: {e}")
        finally:
            conn.close()

# ä¸»ç¨‹å¼
def main():
    create_table()

    keywords = load_keywords()
    if not keywords:
        print("âŒ é—œéµå­—æ¸…å–®ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ keywords.txt")
        return

    # å–å¾—ä»Šå¤©æ—¥æœŸï¼Œæ ¼å¼ç‚º YYYY-MM-DD
    today = date.today().isoformat()

    for keyword in keywords:
        print(f"ğŸ” è™•ç†é—œéµå­—: {keyword}")
        articles = fetch_article_links(keyword)
        for article in articles:
            print(f"ğŸ“„ è™•ç†æ–‡ç« : {article['title']} | URL: {article['url']}")
            article_data = parse_article(article["url"])
            if article_data:
                for comment_data in article_data["comments"]:
                    save_to_db(
                        article_data["title"],
                        article_data["content"],
                        comment_data["comment"],
                        comment_data["sentiment_score"],
                        "ptt",
                        keyword,
                        today
                    )

    print("âœ… æ‰€æœ‰é—œéµå­—è™•ç†å®Œæˆ")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç¨‹å¼è¢«ä¸­æ–·ï¼ŒçµæŸåŸ·è¡Œã€‚")
